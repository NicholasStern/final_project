{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import gen_states, evaluate_agent, evaluate_agent_advanced\n",
    "from Baseline_agent.baseline_agent import BaselineAgent\n",
    "from linearAQ_agent.linearAQ import LinearAQ\n",
    "from NNQ_agent.NNQ import NNQ\n",
    "from Q_agent.mdp import MDP, TabularQ, Q_learn, epsilon_greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Initializations \n",
    "path = 'histories/Apple_cleaned.csv'\n",
    "window_size = 5  # time window size\n",
    "history_size = 3  # number of previous days to consider\n",
    "train, val, test = gen_states(path, window_size, history_size)  # data\n",
    "actions = ['buy', 'wait']  # action space\n",
    "epsilon = .05  # amount of randomness\n",
    "discount = 1  # discount factor\n",
    "alpha = .1\n",
    "num_layers = 15  # number of layers in neural net\n",
    "num_units = 9  # number of nodes per layer in neural net\n",
    "c = -1 # constant for baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q agent initialization\n",
    "def gen_Qhist(path):\n",
    "    # imports history from csv\n",
    "    df = pd.read_csv(path)\n",
    "    hist = df.Close - df.Open\n",
    "    hist = hist.apply(np.sign)\n",
    "    hist[hist == -1] = 0  # no price change is considered a decrease\n",
    "    hist = [int(h) for h in hist]\n",
    "    return hist\n",
    "\n",
    "def gen_Qstates(h):\n",
    "    # generates possible states for a history window h\n",
    "    states = [('T')]\n",
    "    for i in range(2**(h+1)):\n",
    "        b = bin(i)[2:]\n",
    "        l = len(b)\n",
    "        b = str(0) * ((h+1) - l) + b\n",
    "        states.append(tuple([int(i) for i in b]))\n",
    "\n",
    "    return states\n",
    "\n",
    "hist = gen_Qhist(path)\n",
    "p = np.copy(history_size)  # pointer index to history (start at 4th element so we have a history window\n",
    "states = gen_Qstates(history_size)\n",
    "actions = ['buy', 'wait']\n",
    "start = tuple([hist[i] for i in range(history_size)])\n",
    "reward = 1  \n",
    "wait_penalty = -.1  \n",
    "\n",
    "# transition_model: function from (state, action) to return the next state at point \"p+1\" in the history\n",
    "def transition_model(state, action, p):\n",
    "    if p == len(hist):\n",
    "        return None\n",
    "    elif action == 'buy':\n",
    "        return ('T')  # signifying terminal state has been reached\n",
    "    else:\n",
    "        new_state = list(state[1:])\n",
    "        new_state.append(hist[p])\n",
    "        return tuple(new_state)\n",
    "\n",
    "# reward_fn: function from (state, action) to real-valued reward at point \"p\" in the history\n",
    "def reward_fn(state, action, p):\n",
    "    if p == len(hist):  # if we have reached the end of the data\n",
    "        return None\n",
    "    elif state == ('T'):  # if terminal state\n",
    "        return 0\n",
    "    elif action == 'buy' and hist[p] == 1:  # if stock went up after buying\n",
    "        return reward\n",
    "    elif action == 'buy' and hist[p] == 0:  # if stock went down after buying\n",
    "        return -reward\n",
    "    else:\n",
    "        return wait_penalty\n",
    "    \n",
    "# Function to Transfer Q agent to time window framework\n",
    "\n",
    "def gen_Qepisodes(path, window_size, history_size):\n",
    "    # read in data\n",
    "    df = gen_Qhist(path)\n",
    "\n",
    "    df_split = np.array([df[i-history_size:i+window_size] for i in range(\n",
    "        history_size, len(df) - window_size - 1, window_size)][:-1])\n",
    "\n",
    "    result_states = []\n",
    "    for episode in df_split:\n",
    "        episode_states = []\n",
    "        for t in range(window_size):\n",
    "            episode_states.append(tuple(episode[t:t+history_size+1].reshape(-1)))\n",
    "        result_states.append(episode_states)\n",
    "\n",
    "    # split into train/val/test (80%, 10%, 10%)\n",
    "    train = result_states[:int(.8 * len(result_states))]\n",
    "    val = result_states[int(.8* len(result_states)):int(.9 * len(result_states))]\n",
    "    test = result_states[int(.9 * len(result_states)):]\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "stock_agent = MDP(states, actions, transition_model, reward_fn, p, hist, history_size, discount)\n",
    "Q = TabularQ(stock_agent.states, stock_agent.actions)\n",
    "Q, actions = Q_learn(stock_agent, Q, iters=2*len(hist[p:-1])+1, eps = epsilon) \n",
    "train_Q, val_Q, test_Q = gen_Qepisodes(path, window_size, history_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to evaluate Q agent in time window framework\n",
    "\n",
    "def evaluate_Qagent(agent, states_data, states_new, window_size, verbose=True):\n",
    "    \"\"\"\n",
    "    This evaluation is based on how much the close price is lower when the\n",
    "    agent decides to buy compared to the initial price of the time window.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    never_bought_count = 0\n",
    "    time_bought = np.zeros(window_size + 1)\n",
    "    for episode, episode_new in zip(states_data, states_new):\n",
    "        for t, (state, state_new) in enumerate(zip(episode, episode_new)):\n",
    "            if t == len(episode) - 1:\n",
    "                # You have to buy at last time frame if didn't buy before\n",
    "                action = \"buy\"\n",
    "                never_bought_count += 1\n",
    "                time_bought[window_size] += 1\n",
    "                scores.append(-state_new[-2])\n",
    "\n",
    "                break\n",
    "            else:\n",
    "                # Get the best action for this state\n",
    "                action = epsilon_greedy(agent, state, eps=0)\n",
    "                if action == \"buy\":\n",
    "                    scores.append(-state_new[-2])\n",
    "                    time_bought[t] += 1\n",
    "                    break\n",
    "    score = np.mean(scores)\n",
    "    proportion_no_action = never_bought_count / len(states_data) * 100\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Average score for the agent is {} and doesn't buy in {}% of the cases.\".format(\n",
    "            score, proportion_no_action))\n",
    "        for t, c in enumerate(time_bought):\n",
    "            if t < window_size:\n",
    "                print(\"t=%i  Bought %i times.\" % (t, c))\n",
    "            else:\n",
    "                print(\"Did not buy %i times.\" % c)\n",
    "\n",
    "    return score, proportion_no_action, time_bought\n",
    "\n",
    "def evaluate_Qagent_function(agent, states_data, verbose):\n",
    "    return evaluate_Qagent(agent, states_data, test, window_size, verbose=verbose)\n",
    "\n",
    "def reset_Qagent(agent):\n",
    "    hist = gen_Qhist(path)\n",
    "    p = np.copy(history_size)\n",
    "    states = gen_Qstates(history_size)\n",
    "    actions = ['buy', 'wait']\n",
    "\n",
    "    stock_agent = MDP(states, actions, transition_model, reward_fn, p,\n",
    "                      hist, history_size, discount)\n",
    "    agent = TabularQ(stock_agent.states, stock_agent.actions)\n",
    "    agent, _ = Q_learn(stock_agent, agent, iters=2 * len(hist[p:-1]) + 1,\n",
    "                       eps=epsilon)  # setting eps = 0 means no epsilon-greedy\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Evaluation (Takes a while)\n",
    "base_agent = BaselineAgent(c)\n",
    "linear_agent = LinearAQ('train', actions, train, epsilon, discount, alpha)\n",
    "nn_agent = NNQ('train', actions, train, epsilon, discount, num_layers, num_units)\n",
    "\n",
    "print('Evaluating Baseline Agent:')\n",
    "base_profit, base_noaction, base_timebought = evaluate_agent(base_agent, val, verbose=False)\n",
    "\n",
    "print('Evaluating Q-learning Agent:')\n",
    "q_profits = evaluate_agent_advanced(Q, test_Q, n=10, evaluate_agent_function=evaluate_Qagent_function, \n",
    "                                    reset_agent_function=reset_Qagent, verbose=False)\n",
    "\n",
    "print('Evaluating Linear Agent:')\n",
    "linear_profits = evaluate_agent_advanced(linear_agent, test, n=10, verbose=True)\n",
    "\n",
    "print('Evaluating NN Agent:')\n",
    "nn_profits = evaluate_agent_advanced(nn_agent, test, n=10, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.hist(linear_profits, color='b', alpha=.3, label='Linear Agent')\n",
    "ax.hist(q_profits, color='g', alpha=.3, label='Q Agent')\n",
    "ax.hist(nn_profits, color='r', alpha=.3, label='NN agent')\n",
    "ax.axvline(x=base_profit, color='k', label = 'Baseline')\n",
    "ax.set_xlabel('Net Profit ($)', fontsize=15)\n",
    "ax.set_ylabel('Counts', fontsize=15)\n",
    "ax.set_title('Agent Performance Comparison', fontsize=20)\n",
    "ax.legend(fontsize=10)\n",
    "plt.savefig('images/agent_comparison.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
