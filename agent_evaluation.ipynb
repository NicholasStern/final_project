{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import gen_states, evaluate_agent, evaluate_agent_advanced\n",
    "from Baseline_agent.baseline_agent import BaselineAgent\n",
    "from linearAQ_agent.linearAQ import LinearAQ\n",
    "from NNQ_agent.NNQ import NNQ\n",
    "from Q_agent.mdp import MDP, TabularQ, Q_learn, epsilon_greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Initializations \n",
    "path = 'histories/Apple_cleaned.csv'\n",
    "window_size = 5  # time window size\n",
    "history_size = 3  # number of previous days to consider\n",
    "train, val, test = gen_states(path, window_size, history_size)  # data\n",
    "actions = ['buy', 'wait']  # action space\n",
    "epsilon = .05  # amount of randomness\n",
    "discount = 1  # discount factor\n",
    "alpha = .1\n",
    "num_layers = 15  # number of layers in neural net\n",
    "num_units = 9  # number of nodes per layer in neural net\n",
    "c = -1 # constant for baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q agent initialization\n",
    "def gen_Qhist(path):\n",
    "    # imports history from csv\n",
    "    df = pd.read_csv(path)\n",
    "    hist = df.Close - df.Open\n",
    "    hist = hist.apply(np.sign)\n",
    "    hist[hist == -1] = 0  # no price change is considered a decrease\n",
    "    hist = [int(h) for h in hist]\n",
    "    return hist\n",
    "\n",
    "def gen_Qstates(h):\n",
    "    # generates possible states for a history window h\n",
    "    states = [('T')]\n",
    "    for i in range(2**(h+1)):\n",
    "        b = bin(i)[2:]\n",
    "        l = len(b)\n",
    "        b = str(0) * ((h+1) - l) + b\n",
    "        states.append(tuple([int(i) for i in b]))\n",
    "\n",
    "    return states\n",
    "\n",
    "hist = gen_Qhist(path)\n",
    "p = np.copy(history_size)  # pointer index to history (start at 4th element so we have a history window\n",
    "states = gen_Qstates(history_size)\n",
    "actions = ['buy', 'wait']\n",
    "start = tuple([hist[i] for i in range(history_size)])\n",
    "reward = 1  \n",
    "wait_penalty = -.1  \n",
    "\n",
    "# transition_model: function from (state, action) to return the next state at point \"p+1\" in the history\n",
    "def transition_model(state, action, p):\n",
    "    if p == len(hist):\n",
    "        return None\n",
    "    elif action == 'buy':\n",
    "        return ('T')  # signifying terminal state has been reached\n",
    "    else:\n",
    "        new_state = list(state[1:])\n",
    "        new_state.append(hist[p])\n",
    "        return tuple(new_state)\n",
    "\n",
    "# reward_fn: function from (state, action) to real-valued reward at point \"p\" in the history\n",
    "def reward_fn(state, action, p):\n",
    "    if p == len(hist):  # if we have reached the end of the data\n",
    "        return None\n",
    "    elif state == ('T'):  # if terminal state\n",
    "        return 0\n",
    "    elif action == 'buy' and hist[p] == 1:  # if stock went up after buying\n",
    "        return reward\n",
    "    elif action == 'buy' and hist[p] == 0:  # if stock went down after buying\n",
    "        return -reward\n",
    "    else:\n",
    "        return wait_penalty\n",
    "    \n",
    "# Function to Transfer Q agent to time window framework\n",
    "\n",
    "def gen_Qepisodes(path, window_size, history_size):\n",
    "    # read in data\n",
    "    df = gen_Qhist(path)\n",
    "\n",
    "    df_split = np.array([df[i-history_size:i+window_size] for i in range(\n",
    "        history_size, len(df) - window_size - 1, window_size)][:-1])\n",
    "\n",
    "    result_states = []\n",
    "    for episode in df_split:\n",
    "        episode_states = []\n",
    "        for t in range(window_size):\n",
    "            episode_states.append(tuple(episode[t:t+history_size+1].reshape(-1)))\n",
    "        result_states.append(episode_states)\n",
    "\n",
    "    # split into train/val/test (80%, 10%, 10%)\n",
    "    train = result_states[:int(.8 * len(result_states))]\n",
    "    val = result_states[int(.8* len(result_states)):int(.9 * len(result_states))]\n",
    "    test = result_states[int(.9 * len(result_states)):]\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "stock_agent = MDP(states, actions, transition_model, reward_fn, p, hist, history_size, discount)\n",
    "Q = TabularQ(stock_agent.states, stock_agent.actions)\n",
    "Q, _ = Q_learn(stock_agent, Q, iters=2*len(hist[p:-1])+1, eps = epsilon) \n",
    "train_Q, val_Q, test_Q = gen_Qepisodes(path, window_size, history_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to evaluate Q agent in time window framework\n",
    "\n",
    "def evaluate_Qagent(agent, states_data, states_new, window_size, verbose=True):\n",
    "    \"\"\"\n",
    "    This evaluation is based on how much the close price is lower when the\n",
    "    agent decides to buy compared to the initial price of the time window.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    never_bought_count = 0\n",
    "    time_bought = np.zeros(window_size + 1)\n",
    "    for episode, episode_new in zip(states_data, states_new):\n",
    "        for t, (state, state_new) in enumerate(zip(episode, episode_new)):\n",
    "            if t == len(episode) - 1:\n",
    "                # You have to buy at last time frame if didn't buy before\n",
    "                action = \"buy\"\n",
    "                never_bought_count += 1\n",
    "                time_bought[window_size] += 1\n",
    "                scores.append(-state_new[-2])\n",
    "\n",
    "                break\n",
    "            else:\n",
    "                # Get the best action for this state\n",
    "                action = epsilon_greedy(agent, state, eps=0)\n",
    "                if action == \"buy\":\n",
    "                    scores.append(-state_new[-2])\n",
    "                    time_bought[t] += 1\n",
    "                    break\n",
    "    score = np.mean(scores)\n",
    "    proportion_no_action = never_bought_count / len(states_data) * 100\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Average score for the agent is {} and doesn't buy in {}% of the cases.\".format(\n",
    "            score, proportion_no_action))\n",
    "        for t, c in enumerate(time_bought):\n",
    "            if t < window_size:\n",
    "                print(\"t=%i  Bought %i times.\" % (t, c))\n",
    "            else:\n",
    "                print(\"Did not buy %i times.\" % c)\n",
    "\n",
    "    return score, proportion_no_action, time_bought\n",
    "\n",
    "def evaluate_Qagent_function(agent, states_data, verbose):\n",
    "    return evaluate_Qagent(agent, states_data, test, window_size, verbose=verbose)\n",
    "\n",
    "def reset_Qagent(agent):\n",
    "    hist = gen_Qhist(path)\n",
    "    p = np.copy(history_size)\n",
    "    states = gen_Qstates(history_size)\n",
    "    actions = ['buy', 'wait']\n",
    "\n",
    "    stock_agent = MDP(states, actions, transition_model, reward_fn, p,\n",
    "                      hist, history_size, discount)\n",
    "    agent = TabularQ(stock_agent.states, stock_agent.actions)\n",
    "    agent, _ = Q_learn(stock_agent, agent, iters=2 * len(hist[p:-1]) + 1,\n",
    "                       eps=epsilon)  # setting eps = 0 means no epsilon-greedy\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading linear agent...\n",
      "Loading NNQ agent...\n",
      "Evaluating Baseline Agent...\n",
      "Evaluating Q-learning Agent...\n",
      "Evaluating Linear Agent...\n",
      "Evaluating NN Agent...\n",
      "1/5\n",
      "2/5\n",
      "3/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Ecole\\Harvard\\CS182\\cs182-final_project\\utils.py\u001b[0m in \u001b[0;36mevaluate_agent_advanced\u001b[1;34m(agent, states_data, n, evaluate_agent_function, reset_agent_function, verbose)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproportion_no_action\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_bought\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_agent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Ecole\\Harvard\\CS182\\cs182-final_project\\NNQ_agent\\NNQ.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, iters)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_prime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms_prime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Ecole\\Harvard\\CS182\\cs182-final_project\\NNQ_agent\\NNQ.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, a, s, sp, r)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;33m-\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdesired\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         '''\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscount\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnfeats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Ecole\\Harvard\\CS182\\cs182-final_project\\NNQ_agent\\NNQ.py\u001b[0m in \u001b[0;36mvalue\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Ecole\\Harvard\\CS182\\cs182-final_project\\NNQ_agent\\NNQ.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Ecole\\Harvard\\CS182\\cs182-final_project\\NNQ_agent\\NNQ.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, a, s)\u001b[0m\n\u001b[0;32m     76\u001b[0m         '''\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnfeats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1876\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1877\u001b[0m       return training_arrays.predict_loop(\n\u001b[1;32m-> 1878\u001b[1;33m           self, x, batch_size=batch_size, verbose=verbose, steps=steps)\n\u001b[0m\u001b[0;32m   1879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1880\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, inputs, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 2986\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2988\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Agent Evaluation (Takes a while)\n",
    "base_agent = BaselineAgent(c)\n",
    "print(\"Loading linear agent...\")\n",
    "linear_agent = LinearAQ('train', actions, train, epsilon, discount, alpha)\n",
    "print(\"Loading NNQ agent...\")\n",
    "nn_agent = NNQ('train', actions, train, epsilon, discount, num_layers, num_units)\n",
    "n = 5\n",
    "\n",
    "print('Evaluating Baseline Agent...')\n",
    "base_profit, base_noaction, base_timebought = evaluate_agent(base_agent, val, verbose=False)\n",
    "\n",
    "print('Evaluating Q-learning Agent...')\n",
    "q_profits = evaluate_agent_advanced(Q, test_Q, n=n, evaluate_agent_function=evaluate_Qagent_function, \n",
    "                                    reset_agent_function=reset_Qagent, verbose=False)\n",
    "\n",
    "print('Evaluating Linear Agent...')\n",
    "linear_profits = evaluate_agent_advanced(linear_agent, test, n=n, verbose=False)\n",
    "\n",
    "print('Evaluating NN Agent...')\n",
    "nn_profits = evaluate_agent_advanced(nn_agent, test, n=n, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linear_profits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-62938a05a541>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear_profits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Linear Agent'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_profits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'g'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Q Agent'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_profits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'NN agent'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxvline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbase_profit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'k'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Baseline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'linear_profits' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.hist(linear_profits, color='b', alpha=.3, label='Linear Agent')\n",
    "ax.hist(q_profits, color='g', alpha=.3, label='Q Agent')\n",
    "ax.hist(nn_profits, color='r', alpha=.3, label='NN agent')\n",
    "ax.axvline(x=base_profit, color='k', label = 'Baseline')\n",
    "ax.set_xlabel('Net Profit ($)', fontsize=15)\n",
    "ax.set_ylabel('Counts', fontsize=15)\n",
    "ax.set_title('Agent Performance Comparison', fontsize=20)\n",
    "ax.legend(fontsize=10)\n",
    "plt.savefig('images/agent_comparison.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
