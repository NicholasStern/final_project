{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "## Approximate Q Learning for Stock Trend Prediction\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "def make_nn(state_dim, num_hidden_layers, num_units):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_units, input_dim = state_dim, activation='relu'))\n",
    "    for i in range(num_hidden_layers-1):\n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam())\n",
    "    return model\n",
    "\n",
    "class NNQ:\n",
    "    def __init__(self, mode, actions, states, epsilon, discount, num_layers = 3, num_units = 100):\n",
    "        self.mode = mode  # train or test\n",
    "        self.actions = actions  # list of actions\n",
    "        self.states = states  # market history to walk through in the form of a 2D array\n",
    "        self.epsilon = epsilon  # randomness of actions\n",
    "        self.discount = discount  # discount factor\n",
    "        self.nfeats = self.states.shape[-1]\n",
    "        self.num_layers = num_layers\n",
    "        self.num_units = num_units\n",
    "\n",
    "        # initialize linear model w/ weights dictionary for each action\n",
    "        self.models = dict([(a, make_nn(self.nfeats, self.num_layers, self.num_units)) for a in self.actions])\n",
    "\n",
    "    def switch_mode(self, new_mode, new_states):\n",
    "        self.mode = new_mode\n",
    "        self.states = new_states\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "            - resets the weights of the agent (for repeated training and testing situations)\n",
    "        '''\n",
    "        self.models = dict([(a, make_nn(self.nfeats, self.num_layers, self.num_units)) for a in self.actions])\n",
    "\n",
    "    def reward(self, a):\n",
    "        '''\n",
    "            - Calculates reward to give agent based on action a at timestep t in window w\n",
    "            - Reward is based on regret\n",
    "        '''\n",
    "\n",
    "        if a == 'buy' or self.t == (len(self.states[self.w])-1):  # force agent to buy at end of time frame\n",
    "            choice = self.states[self.w][self.t][-2] # difference in price from chosen day to initial day\n",
    "            best = min([x[-2] for x in self.states[self.w]])  # best close price\n",
    "            regret = choice - best\n",
    "            return -choice, regret\n",
    "        else:\n",
    "            return 0, 0\n",
    "\n",
    "    def value(self, s):\n",
    "        return max(self.predict(a, s) for a in self.actions)\n",
    "\n",
    "    def update(self, a, s, sp, r):\n",
    "        '''\n",
    "            - Takes in action a, state s, next state sp, and reward r\n",
    "            - Training on your network for action a, on a data set made up of a single data point, where the input is\n",
    "            - s and the desired output is t\n",
    "        '''\n",
    "        t = r + self.discount * self.value(sp)\n",
    "        X = np.array(s).reshape(1,self.nfeats)\n",
    "        Y = np.array(t)\n",
    "        self.models[a].fit(X, Y, epochs=1, verbose=0)\n",
    "\n",
    "    def predict(self, a, s):\n",
    "        '''\n",
    "            - Takes in action a and state s\n",
    "            - Performs function approximation to predict q-value\n",
    "            - Returns q-value prediction\n",
    "        '''\n",
    "\n",
    "        return self.models[a].predict(np.array(s).reshape(1, self.nfeats))\n",
    "\n",
    "\n",
    "    def epsilon_greedy(self, s, eps=0.5):\n",
    "        '''\n",
    "            - Takes in state and epsilon and returns action\n",
    "        '''\n",
    "        if self.mode == 'test':\n",
    "            eps = 0\n",
    "\n",
    "        if random.random() < eps:  # True with prob eps, random action\n",
    "            return self.actions[random.randint(0,len(self.actions)-1)]\n",
    "        else:  # False with prob 1-eps, greedy action\n",
    "            q_vals = np.zeros(len(self.actions))\n",
    "            for i, a in enumerate(self.actions):\n",
    "\n",
    "                q_vals[i] = self.predict(a, s)\n",
    "\n",
    "            return self.actions[q_vals.argmax()]\n",
    "\n",
    "    def transition(self, a):\n",
    "        '''\n",
    "            - Takes in t, w and returns next state\n",
    "        '''\n",
    "\n",
    "        self.t += 1  # move to next day\n",
    "        if self.t == len(self.states[self.w]) or a == 'buy':\n",
    "            self.t = 0\n",
    "            self.w += 1\n",
    "            if self.w == len(self.states):\n",
    "                return None  # We are finished traversing state space\n",
    "\n",
    "        return tuple(self.states[self.w][self.t])  # return next state\n",
    "\n",
    "\n",
    "\n",
    "    def learn(self, iters=100):\n",
    "        '''\n",
    "            - Takes in state and epsilon and returns action\n",
    "        '''\n",
    "        self.w = 0  # window index\n",
    "        self.t = 0  # timestep index within window\n",
    "        s = tuple(self.states[self.w][self.t])  # init state\n",
    "        actions = []\n",
    "        profit = []\n",
    "        regret = []\n",
    "        for _ in range(iters):\n",
    "            a = self.epsilon_greedy(s, self.epsilon)\n",
    "            actions.append(a)\n",
    "\n",
    "            r, reg = self.reward(a)\n",
    "            if a == 'buy':\n",
    "                profit.append(r)\n",
    "                regret.append(reg)\n",
    "\n",
    "            s_prime = self.transition(a)\n",
    "\n",
    "            if s_prime is None:\n",
    "                break\n",
    "\n",
    "            if self.mode == 'train':\n",
    "                self.update(a, s, s_prime, r)\n",
    "\n",
    "            s = s_prime\n",
    "\n",
    "        return actions, profit, regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, 1.0)\n",
      "(1.0, 2.0)\n",
      "(1.0, 3.0)\n",
      "(1.0, 4.0)\n",
      "(1.0, 5.0)\n",
      "(1.0, 6.0)\n",
      "(1.0, 7.0)\n",
      "(1.0, 8.0)\n",
      "(1.0, 9.0)\n",
      "(1.0, 10.0)\n",
      "(3.0, 1.0)\n",
      "(3.0, 2.0)\n",
      "(3.0, 3.0)\n",
      "(3.0, 4.0)\n",
      "(3.0, 5.0)\n",
      "(3.0, 6.0)\n",
      "(3.0, 7.0)\n",
      "(3.0, 8.0)\n",
      "(3.0, 9.0)\n",
      "(3.0, 10.0)\n",
      "(5.0, 1.0)\n",
      "(5.0, 2.0)\n",
      "(5.0, 3.0)\n",
      "(5.0, 4.0)\n",
      "(5.0, 5.0)\n",
      "(5.0, 6.0)\n",
      "(5.0, 7.0)\n",
      "(5.0, 8.0)\n",
      "(5.0, 9.0)\n",
      "(5.0, 10.0)\n",
      "(7.0, 1.0)\n",
      "(7.0, 2.0)\n",
      "(7.0, 3.0)\n",
      "(7.0, 4.0)\n",
      "(7.0, 5.0)\n",
      "(7.0, 6.0)\n",
      "(7.0, 7.0)\n",
      "(7.0, 8.0)\n",
      "(7.0, 9.0)\n",
      "(7.0, 10.0)\n",
      "(9.0, 1.0)\n",
      "(9.0, 2.0)\n",
      "(9.0, 3.0)\n",
      "(9.0, 4.0)\n",
      "(9.0, 5.0)\n",
      "(9.0, 6.0)\n",
      "(9.0, 7.0)\n",
      "(9.0, 8.0)\n",
      "(9.0, 9.0)\n",
      "(9.0, 10.0)\n",
      "(11.0, 1.0)\n",
      "(11.0, 2.0)\n",
      "(11.0, 3.0)\n",
      "(11.0, 4.0)\n",
      "(11.0, 5.0)\n",
      "(11.0, 6.0)\n",
      "(11.0, 7.0)\n",
      "(11.0, 8.0)\n",
      "(11.0, 9.0)\n",
      "(11.0, 10.0)\n",
      "(13.0, 1.0)\n",
      "(13.0, 2.0)\n",
      "(13.0, 3.0)\n",
      "(13.0, 4.0)\n",
      "(13.0, 5.0)\n",
      "(13.0, 6.0)\n",
      "(13.0, 7.0)\n",
      "(13.0, 8.0)\n",
      "(13.0, 9.0)\n",
      "(13.0, 10.0)\n",
      "(15.0, 1.0)\n",
      "(15.0, 2.0)\n",
      "(15.0, 3.0)\n",
      "(15.0, 4.0)\n",
      "(15.0, 5.0)\n",
      "(15.0, 6.0)\n",
      "(15.0, 7.0)\n",
      "(15.0, 8.0)\n",
      "(15.0, 9.0)\n",
      "(15.0, 10.0)\n",
      "(17.0, 1.0)\n",
      "(17.0, 2.0)\n",
      "(17.0, 3.0)\n",
      "(17.0, 4.0)\n",
      "(17.0, 5.0)\n",
      "(17.0, 6.0)\n",
      "(17.0, 7.0)\n",
      "(17.0, 8.0)\n",
      "(17.0, 9.0)\n",
      "(17.0, 10.0)\n",
      "(19.0, 1.0)\n",
      "(19.0, 2.0)\n",
      "(19.0, 3.0)\n",
      "(19.0, 4.0)\n",
      "(19.0, 5.0)\n",
      "(19.0, 6.0)\n",
      "(19.0, 7.0)\n",
      "(19.0, 8.0)\n",
      "(19.0, 9.0)\n",
      "(19.0, 10.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import gen_states, evaluate_agent_advanced\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(title, actions, profit, regret):\n",
    "    ac = Counter(actions)\n",
    "    purch = ac['buy'] / len(actions)\n",
    "\n",
    "#     print('{} purchase fraction: {:.4f}'.format(title, purch))\n",
    "#     print('{} avg. profit: {:.4f}'.format(title, np.mean(profit)))\n",
    "#     print('{} avg. regret: {:.4f}'.format(title, np.mean(regret)))\n",
    "    return np.mean(profit), np.mean(regret)\n",
    "\n",
    "## Initialize Parameters\n",
    "window_size = 5\n",
    "history_size = 3\n",
    "train, val, test = gen_states('Apple_cleaned.csv', window_size, history_size)\n",
    "actions = ['buy', 'wait']\n",
    "epsilon = .05\n",
    "discount = 1\n",
    "num_layers = np.linspace(start=1,stop=19,num=10)\n",
    "num_units = np.linspace(start=1,stop=10,num=10)\n",
    "\n",
    "\n",
    "## Pass to Approximate Q-Learning Agent\n",
    "profits = []\n",
    "regrets = []\n",
    "pairs = []\n",
    "for layer in num_layers:\n",
    "    for unit in num_units:\n",
    "        agent = NNQ('train', actions, train, epsilon, discount, int(layer), int(unit))\n",
    "        agent.switch_mode('val', val)\n",
    "        val_actions, val_profit, val_regret = agent.learn()\n",
    "        profit, regret = evaluate('val', val_actions, val_profit, val_regret)\n",
    "        profits.append(profit)\n",
    "        regrets.append(regret)\n",
    "        pairs.append((layer, unit))\n",
    "        print((layer, unit))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15.0, 9.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[np.argmin(np.array(regrets))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.15083458, 0.3550005 , 1.22054141, 1.19574536, 1.3915013 ,\n",
       "       2.0649985 , 1.57742903, 1.45586266, 1.4130013 , 1.18828663,\n",
       "       1.29185796, 1.21953537, 1.30958437, 1.11754728, 1.515626  ,\n",
       "       1.14666733, 2.419998  , 1.51058971, 1.25660074, 1.28280044,\n",
       "       1.3147153 , 1.29471506, 1.30314369, 1.16942927, 1.09884637,\n",
       "       0.9900009 , 1.27200084, 1.29185796, 1.20833414, 1.26757194,\n",
       "       1.28300067, 1.185625  , 1.29457229, 0.81000033, 1.28085821,\n",
       "       1.63807715, 1.64205935, 1.3883345 , 1.1968755 , 1.27600091,\n",
       "       1.28328654, 1.29071507, 1.03928612, 1.27728644, 1.12891957,\n",
       "       2.07      , 1.22014371, 1.3637505 , 1.52444456, 1.34595317,\n",
       "       1.31185799, 1.29071507, 1.26014366, 1.20042937, 1.26085786,\n",
       "       2.2800002 , 1.04650873, 1.17100084, 1.24933343, 1.47166717,\n",
       "       1.33357221, 1.28885807, 1.3010008 , 0.136668  , 0.510002  ,\n",
       "       1.229738  , 1.290001  , 1.29614371, 1.12878107, 0.6675015 ,\n",
       "       1.27757213, 1.27185794, 1.29314373, 1.31371507, 1.32514339,\n",
       "       0.71000167, 2.10000133, 1.29857216, 0.0999985 , 1.8450015 ,\n",
       "       1.28800097, 1.28942951, 1.28014367, 1.22957224, 1.25457236,\n",
       "       1.25728634, 1.27528659, 1.66190529, 2.594997  , 1.34640056,\n",
       "       1.33028657, 1.28528643, 1.06620707, 1.5175    , 1.18909109,\n",
       "       1.32571524, 0.1450005 , 1.48500108, 1.2575721 , 1.29185796])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(regrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
